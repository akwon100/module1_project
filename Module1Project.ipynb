{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1 Project \n",
    "Alice Kwon\n",
    "<br>Part Time\n",
    "<br>Instructor: Eli Thomas\n",
    "<br>Blog URL: https://akwon100.github.io/module1_project_ds-pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Organization:\n",
    "We split each cell with the following labels:\n",
    "1. Import\n",
    "2. Webscraping\n",
    "3. Making Dataframe\n",
    "4. Cleaning Dataframe\n",
    "5. Organizing data\n",
    "6. Visualization\n",
    "For each label we add a small description if necessary\n",
    "\n",
    "For this project we need not use databases HOWEVER, they can be used and is often a good idea to use when\n",
    "handling large amounts of data. Hence at the end after all questions are anwered we show how to create a table \n",
    "and insert the information scraped for those who want to use databases. The scraped data will have a (*) pairing \n",
    "with the cells at the end. \n",
    "\n",
    "Note: This is the final version of the code, hence none of the cells will be run\n",
    "as they have already been run and tested in a draft version\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib.parse\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WEBSCRAPING: FUNCTIONS IMDB VIA API \n",
    "#Getting movies by title \n",
    "\n",
    "# Constants\n",
    "API_KEY = 'k_PtQ8V2FQ/'\n",
    "API_URL_SEARCH_MOVIE = 'https://imdb-api.com/en/API/SearchMovie/' + API_KEY\n",
    "API_URL_SEARCH_TITLE = 'https://imdb-api.com/en/API/Title/' + API_KEY\n",
    "\n",
    "# Logging\n",
    "movie_number = 0\n",
    "found_id_count = 0\n",
    "id_not_found = 0\n",
    "movie_not_found = 0\n",
    "total_added = 0\n",
    "\n",
    "\n",
    "def fetchMovieByTitle(movie_title, movie_year, keys, catch):\n",
    "    '''\n",
    "    Parameters: movie_title: <str> string to find movie id  \n",
    "                movie_year: <str or int> string or int to find movie id \n",
    "                keys: <list or tuple> list of keys of dictionary of response object\n",
    "                catch: <list or tuple> an empty list to catch results\n",
    "    Returns: response object and appends to catch\n",
    "    '''\n",
    "    encoded_movie_title = urllib.parse.quote(str(movie_title), safe='/', encoding=None, errors=None)\n",
    "    url = API_URL_SEARCH_MOVIE + encoded_movie_title\n",
    "\n",
    "    response = requests.request(\"GET\", url)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(response.status_code, 'cannot find movie:', movie_title)\n",
    "\n",
    "        # Logging\n",
    "        global id_not_found\n",
    "        id_not_found = id_not_found + 1\n",
    "        return False\n",
    "    else:\n",
    "        print(response.status_code, 'movie found', movie_title)\n",
    "        data = json.loads(response.text)\n",
    "        getMovieIdFromResults(data.get('results'), movie_title, movie_year, keys, catch)\n",
    "\n",
    "        # Logging\n",
    "        global found_id_count\n",
    "        found_id_count = found_id_count + 1\n",
    "        return True\n",
    "\n",
    "def getMovieIdFromResults(api_result, movie_title, movie_year, keys, catch):\n",
    "    '''\n",
    "    Parameters: api_result:<dict> response obj\n",
    "                movie_title: <str> string to find movie id  \n",
    "                movie_year: <str or int> string or int to find movie id \n",
    "                keys: <list or tuple> list of keys of dictionary of response object\n",
    "                catch: <list or tuple> an empty list to catch results\n",
    "    Returns: response object and appends to catch\n",
    "    '''\n",
    "    for result in api_result:\n",
    "        if type(result) is dict:\n",
    "            get_title = result.get('title')\n",
    "            get_year = result.get('description')\n",
    "            get_id = result.get('id')\n",
    "\n",
    "            if str(movie_title) in str(get_title) and str(movie_year) in str(get_year):\n",
    "                fetchMovieDataFromId(get_id, keys, catch)\n",
    "                return True\n",
    "            else:\n",
    "                print('the movie', str(movie_title), 'was not found')\n",
    "\n",
    "                # Logging\n",
    "                global movie_not_found\n",
    "                movie_not_found = movie_not_found + 1\n",
    "                return True\n",
    "        else:\n",
    "            print('the result is not a dict')\n",
    "            return True\n",
    "\n",
    "def fetchMovieDataFromId(movie_id, keys, catch):\n",
    "    '''\n",
    "    Parameters: movie_id: <str> imdb movie id\n",
    "                keys: <list or tuple> list of keys of dictionary of response object\n",
    "                catch: <list or tuple> an empty list to catch results\n",
    "    Returns: response object and appedns to catch\n",
    "    '''\n",
    "    global total_added\n",
    "    #all data is present\n",
    "    if total_added >= 2000:\n",
    "        return True\n",
    "\n",
    "    url = API_URL_SEARCH_TITLE + movie_id\n",
    "    response = requests.request(\"GET\", url)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(response.status_code, 'cannot find movie for id:', movie_id)\n",
    "        return False\n",
    "    else:\n",
    "        results = json.loads(response.text)\n",
    "        log_results = []\n",
    "        result = [movie_id] + [results.get(key) for key in keys if results.get(key) != '']\n",
    "        catch.append(tuple(result))\n",
    "        log_results.append(result)    \n",
    "\n",
    "        # Logging\n",
    "        total_added = total_added + 1\n",
    "        print('appending', log_results, total_added)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WEBSCRAPING: Functions scraping IMDB via beautiful soup\n",
    "\n",
    "#Letting sites know we are a browser\n",
    "def iAmBrowser(url):\n",
    "    headers = requests.utils.default_headers()\n",
    "    headers.update({'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0',})\n",
    "    page = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(page.content ,'lxml')\n",
    "    return soup\n",
    "\n",
    "#making dictionary to create dataframe with metascore, metacritic, maturity rating, imdb rating, num of imdb votes \n",
    "def making_list(movie_id, catch):\n",
    "    '''\n",
    "    Parameters: movie_id: <str> imdb movie id\n",
    "                catch:<list or tuple> empty list to catch result\n",
    "    Returns: <list> catch\n",
    "    '''\n",
    "    metascore = retrieveMetascore(movie_id)[0]\n",
    "    metacritic = retrieveMetascore(movie_id)[1]\n",
    "    mrating = retrieveMaturityRating(movie_id)\n",
    "    imdbScore = retrieveImdbrating(movie_id)[0]\n",
    "    imdbVotes = retrieveImdbrating(movie_id)[1]\n",
    "    result = (movie_id, metascore, metacritic, mrating, imdbScore, imdbVotes)\n",
    "    catch.append(result)\n",
    "    return catch\n",
    "\n",
    "#retrieving metascore for each movie \n",
    "def retrieveMetascore(movie_id):\n",
    "    '''\n",
    "    Parameters: movie_id: <str> string to get url for movie\n",
    "    Returns: metascore:<str> metascore of movie and number of metacritics\n",
    "    '''\n",
    "    url = 'https://www.imdb.com/title/' + movie_id + '/criticreviews?ref_=tt_ov_rt'\n",
    "    soup = iAmBrowser(url)\n",
    "    \n",
    "    metascore = soup.find('span', {'itemprop': 'ratingValue'})\n",
    "    if metascore:\n",
    "        metascore = metascore.text.strip()\n",
    "        #print('appending metascore:' + metascore)\n",
    "    \n",
    "    else:\n",
    "        metascore = 'None'\n",
    "        #print('Not found')\n",
    "    \n",
    "    metacritics = soup.find('span', {'itemprop': 'ratingCount'})\n",
    "    if metacritics:\n",
    "        metacritics = metacritics.text.strip()\n",
    "        #print('appending metacritic:' + metacritics)\n",
    "    else:\n",
    "        metacritics = 'None'\n",
    "        #print('Not found')\n",
    "    \n",
    "    return metascore, metacritics\n",
    "\n",
    "#retrieving maturity rating for each movie\n",
    "def retrieveMaturityRating(movie_id):\n",
    "    '''\n",
    "    Parameters: movie_id:<str> movie id\n",
    "    Returns: <str> maturity rating\n",
    "    '''\n",
    "    url = 'https://www.imdb.com/title/' + movie_id + '/parentalguide?ref_=tt_stry_pg'\n",
    "    soup = iAmBrowser(url)\n",
    "    \n",
    "    MaturityRating = soup.find(id='mpaa-rating')\n",
    "    \n",
    "    if MaturityRating:\n",
    "        MaturityRating = MaturityRating.find_all(\"td\")[1].string.split()[:2]\n",
    "        #print('appending Mrating')\n",
    "        \n",
    "    else:\n",
    "        MaturityRating = 'None'\n",
    "        #print('Not found')\n",
    "        \n",
    "    return listToString(MaturityRating)\n",
    "\n",
    "#retrieving Imdb rating and number of votes for each movie \n",
    "def retrieveImdbrating(movie_id):\n",
    "    '''\n",
    "    Parameters: movie_id:<str> movie id\n",
    "    Returns: <str> imdb score and number of imdb voters\n",
    "    '''\n",
    "    url = \"https://www.imdb.com/title/\" + movie_id + \"/ratings\"\n",
    "    soup = iAmBrowser(url)\n",
    "    \n",
    "    numVotes = soup.find('div',{'class': 'allText'})\n",
    "    if numVotes:\n",
    "        numVotes = numVotes.text.strip()\n",
    "        #print('appending numVotes')\n",
    "        \n",
    "    else:\n",
    "        numVotes = 'None'\n",
    "        #print('Not found')\n",
    "        \n",
    "    rating = soup.find('div',{'class': 'allText'})\n",
    "    if rating:\n",
    "        rating = rating.text.strip()\n",
    "        #print('appending rating')\n",
    "    \n",
    "    else:\n",
    "        rating = 'None'\n",
    "        #print('Not found')\n",
    "    \n",
    "    return numVotes.split()[0], listToString(rating.split()[-3:])\n",
    "\n",
    "def listToString(s):   \n",
    "    str1 = \"\"   \n",
    "    for item in s:  \n",
    "        str1 += ' ' + item     \n",
    "    return str1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WEBSCRAPING: Functions scraping IMDB via beautiful soup\n",
    "\n",
    "#retrieving imdb movie id for each movie from top 100 of each year\n",
    "def top_100(url, catch):\n",
    "    '''\n",
    "    Parameters: url: <url> url to request\n",
    "                catch: <list or tuple> empty list or tuple to catch results\n",
    "    Returns: appends movie ids to catch\n",
    "    '''\n",
    "    soup = iAmBrowser(url)\n",
    "    Movie = soup.find('div', attrs = {'class': 'lister list detail sub-list'})\n",
    "    Movie = Movie.find('div', attrs = {'class' : 'lister-list'})\n",
    "    items = Movie.find_all('div', class_= 'lister-item mode-detail')\n",
    "    \n",
    "    for item in items:\n",
    "        movie_id = item.find('div', attrs={'class': 'lister-item-image ribbonize'})\n",
    "        movie_id = str(item)\n",
    "        for strings in str(movie_id).split():\n",
    "            if 'data-tconst' in strings:\n",
    "                movie_id = strings.split('=')\n",
    "                movie_id = movie_id[1].split('>')\n",
    "                movie_id = movie_id[0].strip('\\\"')\n",
    "                catch.append(movie_id)\n",
    "    #catch = list(dict.fromkeys(catch))\n",
    "    #return catch        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WEBSCRAPING: functions scraping IMDB via beautiful soup\n",
    "\n",
    "#function which retrieves votes \n",
    "def retrieveVotes(movie_id):\n",
    "    '''\n",
    "    Parameters: movie_id:<str> movie id\n",
    "    Returns: <list> list of number of votes \n",
    "    '''\n",
    "    url = \"https://www.imdb.com/title/\" + movie_id + \"/ratings\"\n",
    "    soup = iAmBrowser(url)\n",
    "    \n",
    "    table = soup.find('table')\n",
    "    \n",
    "    if not table:\n",
    "        votes_list.append((movie_id , 0, 0, 0, 0, 0, 0, 0, 0, 0, 0))\n",
    "    \n",
    "    result =[]\n",
    "    rows = table.find_all('tr')    \n",
    "    for tr in rows:\n",
    "        votes = tr.find('div',{ 'class':'leftAligned'}).text.strip()\n",
    "        if votes != 'Votes':\n",
    "            votes = int(votes.replace(',', ''))\n",
    "            result.append(votes)\n",
    "            print('appending Votes')\n",
    "        \n",
    "    reverse_votes = result[::-1]\n",
    "    return reverse_votes\n",
    "\n",
    "#function which makes the tuple of votes we've found \n",
    "def makeVotesTuple(movie_id, title, catch):\n",
    "    '''\n",
    "    Parameters: movie_id:<str> movie id \n",
    "                title:<str> title of movie\n",
    "                catch:<list> empty list to catch results\n",
    "    Returns: appends results of retrieveVotes() to list catch\n",
    "    '''\n",
    "    catch.append((movie_id, title) + tuple([i for i in retrieveVotes(movie_id)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Procedure (A):\n",
    "1. We will first scrape the movie ids of 100 movies sorted by popularity for each year for the past ten years\n",
    "2. We then scrape using API: directors, genres, titles, boxoffice, year\n",
    "3. We then scrape using beautiful soup: metascore, imdb-score, maturity rating\n",
    "4. We then scrap using beautiful soup: imdb vote chart from 1-10\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WEBSCRAPING: we are going to scrape the movie ids (see procedure(A) 1)\n",
    "#top 100 movies sorted by popularity for each year for past ten years \n",
    "url_2019 = 'https://www.imdb.com/list/ls041214362/?sort=moviemeter,asc&st_dt=&mode=detail&page=1'\n",
    "url_2018 = 'https://www.imdb.com/list/ls047677021/?sort=moviemeter,asc&st_dt=&mode=detail&page=1'\n",
    "url_2017 = 'https://www.imdb.com/list/ls023426386/?sort=moviemeter,asc&st_dt=&mode=detail&page=1'\n",
    "url_2016 = 'https://www.imdb.com/list/ls063924870/?sort=moviemeter,asc&st_dt=&mode=detail&page=1'\n",
    "url_2015 = 'https://www.imdb.com/list/ls073386152/?sort=moviemeter,asc&st_dt=&mode=detail&page=1'\n",
    "url_2014 = 'https://www.imdb.com/list/ls058177122/?sort=moviemeter,asc&st_dt=&mode=detail&page=1'\n",
    "url_2013 = 'https://www.imdb.com/list/ls053040009/?sort=moviemeter,asc&st_dt=&mode=detail&page=1'\n",
    "url_2012 = 'https://www.imdb.com/list/ls006206951/?sort=moviemeter,asc&st_dt=&mode=detail&page=1'\n",
    "url_2011 = 'https://www.imdb.com/list/ls000463584/?sort=moviemeter,asc&st_dt=&mode=detail&page=1'\n",
    "url_2010 = 'https://www.imdb.com/list/ls009654807/?sort=moviemeter,asc&st_dt=&mode=detail&page=1'\n",
    "\n",
    "top_100_2019 = []\n",
    "top_100_2018 = []\n",
    "top_100_2017 = []\n",
    "top_100_2016 = []\n",
    "top_100_2015 = []\n",
    "top_100_2014 = []\n",
    "top_100_2013 = []\n",
    "top_100_2012 = []\n",
    "top_100_2011 = []\n",
    "top_100_2010 = []\n",
    "top_100(url_2019, top_100_2019)  \n",
    "top_100(url_2018, top_100_2018)  \n",
    "top_100(url_2017, top_100_2017)  \n",
    "top_100(url_2016, top_100_2016)  \n",
    "top_100(url_2015, top_100_2015)  \n",
    "top_100(url_2014, top_100_2014)  \n",
    "top_100(url_2013, top_100_2013)  \n",
    "top_100(url_2012, top_100_2012)  \n",
    "top_100(url_2011, top_100_2011)  \n",
    "top_100(url_2010, top_100_2010)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(*) see cell 1 at the end \n",
    "#WEBSCRAPING: we will use the combined ids obtained to retrieve data (see procedure(A) 2) \n",
    "top_100_10yrs = top_100_2019+top_100_2018+top_100_2017+top_100_2016+top_100_2015+top_100_2014+top_100_2013+top_100_2012+top_100_2011+top_100_2010\n",
    "top100_10yrs = []\n",
    "keys = ['title', 'genres', 'directors', 'year', 'boxOffice']\n",
    "for movie_id in list(set(top_100_10yrs)):\n",
    "    fetchMovieDataFromId(movie_id, keys, top100_10yrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(*) see cell 2 at the end\n",
    "#WEBSCRAPING: we will use the combined ids obtained to retrieve data (see procedure(A) 3)\n",
    "top100_10yrs2 = []\n",
    "for movie_id in list(set(top_100_10yrs)):\n",
    "    making_list(movie_id, top100_10yrs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(*) see cell 1 at the end \n",
    "#MAKING DATAFRAME: Before proceeding to procedure 4 we will make a dataframe called all.csv (see prcedure(B) 1)\n",
    "\n",
    "#separate boxoffice which is a dictionary with the rest of data\n",
    "CONFIG = {'MOVIE_INFO_COLUMN_NAMES' : ['ids', 'titles', 'genres', 'directors', 'year'],\n",
    "          'MOVIE_RATING_COLUMN_NAMES' : ['ids', 'metascore', 'metacritics', 'mrating', 'imdbvotes', 'imdbScore'],\n",
    "          'VOTES_COLUMN_NAMES' : ['ids', 'title','one', 'two', 'three','four', 'five', 'six', 'seven', 'eight', 'nine', 'ten']\n",
    "         }\n",
    "\n",
    "boxOffice = [x[5] for x in top100_10yrs]\n",
    "listToDf = [(x[0],x[1],x[2],x[3], x[4]) for x in top100_10yrs]\n",
    "\n",
    "#make each into a dataframe\n",
    "listToDf_df = pd.DataFrame(listToDf, columns = CONFIG.get('MOVIE_INFO_COLUMN_NAMES'))\n",
    "boxoffice_df = pd.DataFrame(boxOffice)\n",
    "\n",
    "#concatenate the two dataframes\n",
    "movie_info_df = pd.concat([listToDf_df, boxoffice_df], axis =1, join = 'inner')\n",
    "\n",
    "#make the dataframe of the retrieved ratings \n",
    "movie_ratings_df = pd.DataFrame(top100_10yrs2, columns = CONFIG.get('MOVIE_RATING_COLUMN_NAMES'))\n",
    "\n",
    "#merge the two dataframes on ids\n",
    "all_df = movie_info_df.merge(movie_ratings_df, on = 'ids')\n",
    "\n",
    "#save this dataframe as all.csv\n",
    "all_df.to_csv(r'<insert path here>/all.csv', mode='w', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WEBSCRAPING: we will use the combined ids obtained to retrieve data (see procedure(A) 4)\n",
    "votes_list = []\n",
    "list_movie_id = all_df['ids']\n",
    "list_titles = all_df['titles']\n",
    "for movie_id, title in zip(list_movie_id, list_titles):\n",
    "    makeVotesTuple(movie_id, title, votes_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Procedure(B):\n",
    "1. We make a dataframe called all_df and save as all.csv which includes data from procedure(A)1,2,3\n",
    "2. We make a dataframe called votes_df and save as votes.csv which includes data from procedure(A)4\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAKING DATAFRAME: making dataframe votes_df and saving as votes.csv (see procedure(B)2)\n",
    "votes_df = pd.DataFrame(votes_list, columns = CONFIG.get('VOTES_COLUMN_NAMES'))\n",
    "\n",
    "#save dataframe\n",
    "votes_df.to_csv(r'<insert path here>/votes.csv', mode='w', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1:\n",
    "#### What is the 'best' genres and maturity rating pair?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Procedure(C):\n",
    "We want to clean data according to how we want to answer our first question\n",
    "1.  a)We organize to see if there are any genres which trend by counting genre for each year. \n",
    "    b)Visualize using a bar graph for each year see if there are any trends in genre.\n",
    "    c)Save image as All_graphs.\n",
    "    d) this will determine which combination will be a good combination for genres\n",
    "\n",
    "2.  a)We calculate average profit for each genre and maturity rating pair.\n",
    "    b)Visualize using bar graph for profit vs genre,maturity rating pair.\n",
    "    c)Save image as Genre_profit.\n",
    "\n",
    "3.  a)We calculate average metascore and imdb score for each genre and maturity rating pair.\n",
    "    b)Visualize using bargraph for score vs genre,maturity rating pair.\n",
    "    c)Save image as Genre_score.\n",
    "\n",
    "4.  a)We calculate average skewness for each genre and maturty rating pair.\n",
    "    b)Visualize using bargraph for skewness vs genre,maturity rating pair.\n",
    "    c)Save image as Genre_skew\n",
    "\n",
    "We take the average since there are no major outliers from observation. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ORGANIZING DATA: function which counts genre per year (see procedure(C)1)\n",
    "genre_dict = {'Action':0, 'Adult':0, 'Adventure':0, 'Animation':0,'Biography':0,\n",
    "              'Comedy':0, 'Crime':0, 'Documentary':0, 'Drama':0, 'Family':0, 'Fantasy':0,\n",
    "              'Film_Noir':0, 'Game_Show':0, 'History':0, 'Horror':0, 'Musical':0, 'Music':0, \n",
    "              'Mystery':0,'News':0, 'Reality_TV':0, 'Romance':0, 'Sci_Fi':0, 'Short':0, 'Sport':0, \n",
    "              'Talk_Show':0,'Thriller':0, 'War':0, 'Western':0}\n",
    "\n",
    "def counting_genre(value):\n",
    "    for key in genre_dict.keys():\n",
    "        if key in value:\n",
    "            genre_dict[key] = genre_dict[key]+ 1\n",
    "            return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ORGANIZING DATA: we separate into each year (see procedure(C)1)\n",
    "all2019_df = top_df[top_df['year']== '2019']\n",
    "all2018_df = all_df[all_df['year']== '2018']\n",
    "all2017_df = all_df[all_df['year']== '2017']\n",
    "all2016_df = all_df[all_df['year']== '2016']\n",
    "all2015_df = all_df[all_df['year']== '2015']\n",
    "all2014_df = all_df[all_df['year']== '2014']\n",
    "all2013_df = all_df[all_df['year']== '2013']\n",
    "all2012_df = all_df[all_df['year']== '2012']\n",
    "all2011_df = all_df[all_df['year']== '2011']\n",
    "all2010_df = all_df[all_df['year']== '2010']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ORGANIZING DATA: counting genre for each year (see procedure(C)1)\n",
    "#NOTE: we will have to run this for each year separately and produce graphs separately \n",
    "all2019_df['genres'].apply(lambda x: counting_genre(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT: for visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sys\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VISUALIZATION: bar graph shows which is most reoccuring genre (see procedure(C)1)\n",
    "#NOTE: we will have to run this for each year separately Look at visualizations folder\n",
    "objects = genre_dict.keys()\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = genre_dict.values()\n",
    "plt.rcParams.update({'font.size': 13})\n",
    "plt.subplots(figsize=(12,9))\n",
    "plt.barh(y_pos, performance, align = 'center', alpha = 1.0)\n",
    "plt.yticks(y_pos, objects)\n",
    "plt.ylabel('Genres')\n",
    "plt.xlabel('Count')\n",
    "plt.title('Genre Popularity in 2019')\n",
    "plt.savefig('Genre_2019.png', bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the next step please skip: there are ways to graph side by side via matplotlib rather than to concatenate images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VISUALIZATION: concatenating all images (see procedure(C)1)\n",
    "#the functions below were borrowed from https://note.nkmk.me/en/python-pillow-concat-images/\n",
    "im1 = Image.open('MoreData/2010_12.jpg')\n",
    "im2 = Image.open('MoreData/2013_15.jpg')\n",
    "im3 = Image.open('MoreData/2016_18.jpg')\n",
    "im4 = Image.open('MoreData/Genre_2019.png')\n",
    "im_list = [im1, im2, im3, im4]\n",
    "\n",
    "\n",
    "#function to concatenate multiple images horizontally\n",
    "def get_concat_h_multi_resize(im_list, resample=Image.BICUBIC):\n",
    "    min_height = min(im.height for im in im_list)\n",
    "    im_list_resize = [im.resize((int(im.width * min_height / im.height), min_height),resample=resample)\n",
    "                      for im in im_list]\n",
    "    total_width = sum(im.width for im in im_list_resize)\n",
    "    dst = Image.new('RGB', (total_width, min_height))\n",
    "    pos_x = 0\n",
    "    for im in im_list_resize:\n",
    "        dst.paste(im, (pos_x, 0))\n",
    "        pos_x += im.width\n",
    "    return dst\n",
    "\n",
    "#function to concatenate multiple images vertically \n",
    "def get_concat_v_multi_resize(im_list, resample=Image.BICUBIC):\n",
    "    min_width = min(im.width for im in im_list)\n",
    "    im_list_resize = [im.resize((min_width, int(im.height * min_width / im.width)),resample=resample)\n",
    "                      for im in im_list]\n",
    "    total_height = sum(im.height for im in im_list_resize)\n",
    "    dst = Image.new('RGB', (min_width, total_height))\n",
    "    pos_y = 0\n",
    "    for im in im_list_resize:\n",
    "        dst.paste(im, (0, pos_y))\n",
    "        pos_y += im.height\n",
    "    return dst\n",
    "\n",
    "#saving image\n",
    "get_concat_v_multi_resize(im_list).save('MoreData/All_graphs.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEANING DATAFRAME: each budget and gross should be a float (see procedure(C)2)\n",
    "all_df['budget'] = all_df['budget'].astype(str)\n",
    "all_df['budget'] = all_df['budget'].apply(lambda x: x.replace('$', ''))\n",
    "all_df['budget'] = all_df['budget'].apply(lambda x: x.replace(',', ''))\n",
    "all_df['budget'] = all_df['budget'].apply(lambda x: x.replace('(estimated)', ''))\n",
    "\n",
    "all_df['grossUSA'] = all_df['grossUSA'].astype(str)\n",
    "all_df['grossUSA'] = all_df['grossUSA'].apply(lambda x: x.replace('$', ''))\n",
    "all_df['grossUSA'] = all_df['grossUSA'].apply(lambda x: x.replace(',', ''))\n",
    "\n",
    "all_df['cumulativeWorldwideGross'] = all_df['cumulativeWorldwideGross'].astype(str)\n",
    "all_df['cumulativeWorldwideGross'] = all_df['cumulativeWorldwideGross'].apply(lambda x: x.replace('$', ''))\n",
    "all_df['cumulativeWorldwideGross'] = all_df['cumulativeWorldwideGross'].apply(lambda x: x.replace(',', ''))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEANING DATAFRAME: function for usd conversion (see procedure(C)2)\n",
    "def convertUSD(value):\n",
    "    if 'EUR' in value:\n",
    "        return float(value.replace('EUR',''))*(1.09)\n",
    "    elif 'NOK' in value:\n",
    "        return float(value.replace('NOK',''))*(0.096)\n",
    "    elif 'JPY' in value:\n",
    "        return float(value.replace('JPY',''))*(0.0093)\n",
    "    elif 'DKK' in value:\n",
    "        return float(value.replace('DKK',''))*(0.15)\n",
    "    elif 'GBP' in value:\n",
    "        return float(value.replace('GBP',''))*(1.25)\n",
    "    elif 'INR' in value:\n",
    "        return float(value.replace('INR',''))*(0.013)\n",
    "    elif 'KRW' in value:\n",
    "        return float(value.replace('KRW',''))*(0.00082)\n",
    "    elif 'CAD' in value:\n",
    "        return float(value.replace('CAD',''))*(0.71)\n",
    "    elif 'AUD' in value:\n",
    "        return float(value.replace('AUD',''))*(0.64)\n",
    "    else:\n",
    "        return value\n",
    "    \n",
    "all_df['budget'] = all_df['budget'].apply(lambda x: convertUSD(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEANING DATAFRAME\n",
    "all_df = all_df[all_df['budget'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEANING DATAFRAME\n",
    "def fillWithNaN(value):\n",
    "    if value == '':\n",
    "        return np.nan\n",
    "    else:\n",
    "        return value\n",
    "all_df['grossUSA'] = all_df['grossUSA'].apply(lambda x: fillWithNaN(x))\n",
    "all_df['cumulativeWorldwideGross'] = all_df['cumulativeWorldwideGross'].apply(lambda x: fillWithNaN(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEANING DATAFRAME\n",
    "all_df['grossUSA'] = all_df['grossUSA'].astype(float)\n",
    "all_df['cumulativeWorldwideGross'] = all_df['cumulativeWorldwideGross'].astype(float)\n",
    "all_df['budget'] = all_df['budget'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEANING DATAFRAME\n",
    "#check if there is correlation to use z-score to fill in grossUSA\n",
    "#YES THERE IS \n",
    "sns.scatterplot(x = 'grossUSA', y = 'cumulativeWorldwideGross', data= all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEANING DATAFRAME\n",
    "#check if its normal distribution:\n",
    "#pearson formula using median \n",
    "def pearsonSkew(median, mean, std):\n",
    "    p = 3*(mean-median)/std\n",
    "    return p\n",
    "\n",
    "dom_med = all_df['grossUSA'].median()\n",
    "dom_mean = all_df['grossUSA'].mean()\n",
    "dom_std = all_df['grossUSA'].std()\n",
    "\n",
    "pearsonSkew(dom_med, dom_mean, dom_std)\n",
    "\n",
    "#1.0856898152057168 not normal distribution hence we will use average ratio to fill in missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEANING DATAFRAME\n",
    "all_df['ratio'] = all_df['grossUSA']/all_df['cumulativeWorldwideGross']\n",
    "mean = all_df['ratio'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEANING DATAFRAME\n",
    "all_df['grossUSA'].fillna(all_df['cumulativeWorldwideGross']* mean, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check correlation to see if one can fill budget \n",
    "sns.scatterplot(x = 'budget', y = 'cumulativeWorldwideGross', data= all_df)\n",
    "#NOT enough correlation: we will NOT fill in budget "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make profit column\n",
    "all_df['profit'] = (0.4*(all_df['cumulativeWorldwideGross'] - all_df['grossUSA']) + 0.6*(all_df['grossUSA']))-all_df['budget']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAKING DATAFRAME: making data frame to separate genres (see procedure(C)2)\n",
    "df_list=[]\n",
    "for index, rows in all_df.iterrows(): \n",
    "    my_list =[rows[i] for i in all_df.columns]  \n",
    "    if len(my_list[2].split(',')) > 1:\n",
    "        for item in my_list[2].split(','): \n",
    "            df_list.append([*my_list[:2],item,*my_list[3:]])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAKING DATAFRAME: making dataframe separted genres (see procedure(C)2)\n",
    "singleGenre_df = pd.DataFrame(df_list, columns = all_df.columns)\n",
    "# we drop information we dont need\n",
    "singleGenre_df = singleGenre_df.drop(['openingWeekendUSA'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEANING DATAFRAME: applying usd conversion (see procedure(C)2)\n",
    "singleGenre_df['budget'] = singleGenre_df['budget'] = singleGenre_df['budget'].apply(lambda x: convertUSD(x))\n",
    "singleGenre_df['budget'] = singleGenre_df['budget'].fillna('')\n",
    "singleGenre_df['budget'] = singleGenre_df['budget'].apply(lambda x: 0 if x == '' else x )\n",
    "singleGenre_df = singleGenre_df['budget'].astype(float)\n",
    "singleGenre_df = singleGenre_df[singleGenre_df['budget'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEANING DATAFRAME: changing all to float (see procedure(C)2)\n",
    "singleGenre_df['profit'] = singleGenre_df['profit'].fillna('')\n",
    "singleGenre_df['profit'] = singleGenre_df['profit'].apply(lambda x: 0 if x == '' else x )\n",
    "singleGenre_df = singleGenre_df[singleGenre_df['profit']!=0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEANING DATAFRAME: We only want movies with a maturty rating (see procedure(C)2) \n",
    "singleGenre_df['mrating'] = singleGenre_df['mrating'].fillna(' N o n e')\n",
    "singleGenre_df = singleGenre_df[singleGenre_df['mrating'] != ' N o n e']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAKING DATAFRAME: groupby genre and maturity rating mean (see procedure(C)2)\n",
    "singleGenre_df.groupby(['genres', 'mrating']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VISUALIZATION: Genre and Maturity-rating vs Profit (see procedure(C)2)\n",
    "df = singleGenre_df.groupby(['genres', 'mrating'])['profit'].mean()\n",
    "ax = df.plot(kind='bar', figsize=(25,13), color=\"indigo\", fontsize=10);\n",
    "ax.set_alpha(0.8)\n",
    "ax.set_title(\"genre and Maturity-rating vs Profit\", fontsize=22)\n",
    "ax.set_ylabel(\"genre and m-rating\", fontsize=15);\n",
    "\n",
    "#saving image\n",
    "plt.savefig('Genre_Profit.png', bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEANING DATAFRAME: We want imdbscore and metascore as floats, (see procedure(C)3)\n",
    "singleGenre_df['imdbScore'] = singleGenre_df['imdbScore'].apply(lambda x: x[:4])\n",
    "singleGenre_df['imdbScore'] = singleGenre_df['imdbScore'].astype(float)\n",
    "\n",
    "#we want imdbScore to be similar to metascore so we multiply by 10 so both scores are out of 100\n",
    "singleGenre_df['imdbScore'] = singleGenre_df['imdbScore'].apply(lambda x: x*10)\n",
    "singleGenre_df['metascore'] = singleGenre_df['metascore'].fillna(0)\n",
    "\n",
    "#We only look at those values which are not 0\n",
    "singleGenre_df['metascore'] = singleGenre_df['metascore'].apply(lambda x: float(x))\n",
    "singleGenre_df = singleGenre_df[singleGenre_df['metascore'] !=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#singleGenre_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAKING DATAFRAME: groupby genre and maturity rating via mean. (see procedure(C)3)\n",
    "singleGenre_df.groupby(['genres', 'mrating']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VISUALIZATION: Genre Maturity rating vs Score (see procedure(C)3)\n",
    "x= singleGenre_df[['genres','mrating','metascore','imdbScore' ]]\n",
    "y= x.set_index(['genres', 'mrating'])\n",
    "z=y.groupby(['genres', 'mrating']).mean()\n",
    "ax = z.plot(kind= 'bar', figsize=(25,8), stacked=True)\n",
    "ax.set_title(\"Genre and Maturity-rating vs Scores\", fontsize=30)\n",
    "ax.set_ylabel(\"Scores\", fontsize=20)\n",
    "\n",
    "#save image\n",
    "ax.set_xlabel(\"Genre and Maturity-rating\", fontsize=15)\n",
    "plt.savefig('Genre_score.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ORGANIZING DATA: functions to calculate skewness for each movie (see procedure(C)4)\n",
    "\n",
    "#calculating Bowleyskewness or Yules coeff\n",
    "def bowleySkew(data, freq):\n",
    "    '''\n",
    "    Parameters: data:<list> list of data\n",
    "                freq:<list> frequency of data\n",
    "    Returns: <float> Bowley's skewness\n",
    "    '''\n",
    "    Q1 = calculateValue(25,data,freq)\n",
    "    Q2 = calculateValue(50,data,freq)\n",
    "    Q3 = calculateValue(75,data,freq)\n",
    "    Yule_coeff = (Q3 + Q1 - 2*Q2)/(Q3-Q1)\n",
    "    return round(Yule_coeff,4)\n",
    "\n",
    "#calculating value num\n",
    "def calculateValue(percentile, data, freq):\n",
    "    '''\n",
    "    Parameters: percentile: <int> percentile\n",
    "                data:<list> data\n",
    "                freq:<list> frequency of data\n",
    "    Returns: <float> value number \n",
    "    '''\n",
    "    cum_freq = cumulativeFrequency(freq)\n",
    "    total = cum_freq[-1]\n",
    "    value_num = (percentile/100)*(total+1)\n",
    "    if type(value_num) == float:\n",
    "        value_num = round(value_num)\n",
    "        if value_num <= cum_freq[0]:\n",
    "            return data[0]\n",
    "        else:\n",
    "            for x in range(0,len(cum_freq)):\n",
    "                if value_num >= cum_freq[x-1] and value_num <= cum_freq[x]:\n",
    "                    return data[x]\n",
    "\n",
    "#calculating cumulative freq\n",
    "def cumulativeFrequency(freq):\n",
    "    '''\n",
    "    Parameters: freq:<list> frequency of data\n",
    "    Returns: <list> cumulative frequency\n",
    "    '''\n",
    "    cum_freq = []\n",
    "    cum_freq.append(freq[0])\n",
    "    for x in range(1, len(freq)):\n",
    "        cum_freq.append(cum_freq[x-1] + freq[x])\n",
    "    return cum_freq\n",
    "        \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAKING DATAFRAME: adding skewness to votes_df (see procedure(C)4)\n",
    "skew_list =[]\n",
    "votes = [1,2,3,4,5,6,7,8,9,10]\n",
    "for index, rows in votes_df.iterrows(): \n",
    "    my_list =[rows.one, rows.two, rows.three, rows.four, rows.five, rows.six, rows.seven, rows.eight, rows.nine, rows.ten] \n",
    "    skew_list.append(bowleySkew(votes, my_list))\n",
    "    \n",
    "votes_df['skewness']= skew_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "votes_df = all_df.merge(votes_df, on = 'ids')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAKING DATAFRAME: function separating into genres again (see procedure(C)4)\n",
    "\n",
    "df_list2=[]\n",
    "for index, rows in votes_df.iterrows(): \n",
    "    my_list =[rows[i] for i in votes_df.columns]  \n",
    "    if len(my_list[2].split(',')) > 1:\n",
    "        for item in my_list[2].split(','): \n",
    "            df_list2.append([*my_list[:2],item,*my_list[3:]]) \n",
    "\n",
    "skewd_df = pd.DataFrame(df_list2, columns = votes_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEANING DATAFRAME: cleaning mrating same as singleGenres (see procedure(C)4)\n",
    "skewd_df['mrating'] = skewd_df['mrating'].fillna(' N o n e')\n",
    "skewd_df.mrating.unique()\n",
    "skewd_df = skewd_df[skewd_df['mrating'] != ' N o n e']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VISUALIZATION: Genre Maturity rating vs skew (see procedure(C)4)\n",
    "df = skewd_df.groupby(['genres','mrating'])['skewness'].mean()\n",
    "ax = df.plot(kind='bar', figsize=(25,8), color=\"indigo\", fontsize=10);\n",
    "ax.set_alpha(0.8)\n",
    "ax.set_title(\"Genre vs skewness\", fontsize=30)\n",
    "ax.set_ylabel(\"Skewness\", fontsize=20)\n",
    "ax.set_xlabel(\"Genre\", fontsize=15)\n",
    "plt.savefig('Genre_skew.png', bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANSWER 1: ((Sci-fi, Action), PG-13), ((Crime, Action), PG), ((Mystery,action),PG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Answer: Via the visualization we get the following information:\n",
    "1. Throughout the years Action seems to appear the most, whereas horror and drama seem to lose trend\n",
    "2. We look at top 5-6 movies from each: genre and maturity rating vs (1: profit, 2: skewness, 3: scores)\n",
    "If a movie appears twice or three times in each we take that as a successful genre and maturty rating pair:\n",
    "\n",
    "What we found: (Sci-fi, PG13),(Crime, PG), (Mystery,PG), (Documentary, rated R)\n",
    "Note: we want to make a movie that can profit off of product placement and possible merchandising.\n",
    "Hence we will discard (Documentary, rated R)\n",
    "\n",
    "Since Action was a reoccuring genre with most count throughout ten years the pair \n",
    "(Sci-fi, action), (Crime, action), (Mystery, action) is most likely a good combination of genres\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question2: \n",
    "### What are the top directors for such genre/maturity-rating pair?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Procedure(D):\n",
    "There is not much difference to the procedures in D with procedures in C\n",
    "1. We identify the directors of genre/maturity-rating pair\n",
    "2. See director vs profit (the procedure is similar to that of Procedure (C)1) save as director_score.png\n",
    "3. directors vs scores (the procedure is similar to that of Procedure (C)2) save as director_profit.png\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAKING DATAFRAME: See procedure(D) 1\n",
    "MysteryPG_df = singleGenre_df[(singleGenre_df['genres'] == ' Mystery')&(singleGenre_df['mrating'] == ' Rated PG')]\n",
    "SciFiPG13_df = singleGenre_df[(singleGenre_df['genres'] == ' Sci-Fi')&(singleGenre_df['mrating'] == ' Rated PG-13')]\n",
    "CrimePG_df = singleGenre_df[(singleGenre_df['genres'] == ' Crime')&(singleGenre_df['mrating'] == ' Rated PG')]\n",
    "\n",
    "pieces = (MysteryPG_df,SciFiPG13_df,CrimePG_df)\n",
    "df_final = pd.concat(pieces, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VISUALIZATION: directors vs score\n",
    "x= df_final[['directors','metascore','imdbScore' ]]\n",
    "y= x.set_index(['directors'])\n",
    "ax = y.plot(kind= 'bar', figsize=(25,8), stacked=True)\n",
    "ax.set_title(\"Directors vs Scores\", fontsize=30)\n",
    "ax.set_ylabel(\"Scores\", fontsize=20)\n",
    "ax.set_xlabel(\"Directors\", fontsize=15)\n",
    "\n",
    "#save image\n",
    "plt.savefig('director_score.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VISUALIZATION: directors vs profit\n",
    "x= df_final[['directors','profit' ]]\n",
    "y= x.set_index(['directors'])\n",
    "ax = y.plot(kind= 'bar', figsize=(25,8), color = 'green')\n",
    "ax.set_title(\"Directors vs profit\", fontsize=30)\n",
    "ax.set_ylabel(\"profit\", fontsize=20)\n",
    "ax.set_xlabel(\"Directors\", fontsize=15)\n",
    "\n",
    "#save image\n",
    "plt.savefig('director_profit.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANSWER 2: Anythony Russo and Joe Russo ((Sci-fi, Action), PG-13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Answer: From our findings it seems Anthony Russo and Joe Russo is a good candidate for director\n",
    "They specialize in Sci-Fi PG-13\n",
    "\n",
    "Note: Most movies if not all movies contain multiple genres, any movie which contains Sci-Fi as one of its genre\n",
    "is a movie of genre Sci-Fi. \n",
    "The movie which awarded Anthony Russo and Joe Russo a successful Sci-Fi PG-13 movie was Avengers:EndGame\n",
    "\n",
    "So one should be careful what this genre/maturty pairing really means \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUESTION 3:\n",
    "### Is there a correlation between scores and profit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Procedure(E):\n",
    "1. Since imdb scores, metascore as well as profit ranges are very different we first normalize using \n",
    "z-score normalization. \n",
    "\n",
    "2. Visualize using a scatter plot the correlation between profit and scores\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ORGANIZING DATA: function calculating z-score. (See procedure(D) 1)\n",
    "#normalize using z-score normalization\n",
    "singleGenre_df.drop_duplicates('ids', keep = 'first')\n",
    "\n",
    "metascore_mean = round(singleGenre_df['metascore'].mean(),3)\n",
    "metascore_std = round(singleGenre_df['metascore'].std(),3)\n",
    "\n",
    "imdb_mean = round(singleGenre_df['imdbScore'].mean(),3)\n",
    "imdb_std = round(singleGenre_df['imdbScore'].std(),3)\n",
    "\n",
    "profit_mean = round(singleGenre_df['profit'].mean(),3)\n",
    "profit_std = round(singleGenre_df['profit'].std(),3)\n",
    "\n",
    "def zScoreNormalization(value, mean, std):\n",
    "    z= (value - mean)/std\n",
    "    return z\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAKING DATAFRAME: making dataframe for score and profit/ applying z-score function (See procedure(D) 1)\n",
    "singleGenre_df['metascore'] = singleGenre_df['metascore'].apply(lambda x: zScoreNormalization(x, metascore_mean, metascore_std))\n",
    "singleGenre_df['imdbScore'] = singleGenre_df['imdbScore'].apply(lambda x: zScoreNormalization(x, imdb_mean, imdb_std))\n",
    "singleGenre_df['profit'] = singleGenre_df['profit'].apply(lambda x: zScoreNormalization(x, profit_mean, profit_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VISUALIZATION: scatter plot, score vs profit (See procedure(D) 1)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "sns.scatterplot(x = 'imdbScore', y = 'profit', data= singleGenre_df, ax=ax)\n",
    "sns.scatterplot(x = 'metascore', y = 'profit', data= singleGenre_df, ax=ax)\n",
    "ax.legend(labels = ['metascore', 'imdbScore'])\n",
    "ax.set_xlabel(\"Scores\", fontsize=15)\n",
    "\n",
    "#save image\n",
    "plt.savefig('score_profit.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANSWER 3: Not exactly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Answer:\n",
    "there is not as much evidence to provide that better scores implies better profit\n",
    "ex: that we've seen Crime PG gives good profit but poor votes\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADDITIONAL VISUALIZATION:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Here are some additional visualization tools for a more technical audience such as:\n",
    "heat maps, cluster maps : which are helpful when trying to see correlations between columns of your datafram\n",
    "lmplot: which will help approximate the line of a correlation\n",
    "violinplot: this is especially nice because it shows distrubition along a box-whisker\n",
    "\n",
    "Other additional plots that are not added:\n",
    "There are many additional plots that one can make according to their particular situation:\n",
    "like pie charts etc. \n",
    "\n",
    "I have not had the need to add these additional charts because bargraphs and scatterplot \n",
    "was all the visualization I needed for a striaghtforward visualization of my data. \n",
    "\n",
    "In general it is a good idea to refrain from more complicated visualizations when presenting to a \n",
    "non-technical audience and very good for a technical audience. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#heat map\n",
    "sns.heatmap(singleGenre_df.corr(),cmap='magma',linecolor='white',linewidths=1)\n",
    "plt.savefig('clustermap.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster map\n",
    "sns.clustermap(singleGenre_df.corr(),cmap='coolwarm',standard_scale=1)\n",
    "plt.savefig('clustermap.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regression plot: helpful when trying to approximate a correlation between two axis\n",
    "sns.lmplot(x='imdbScore',y='profit',data=singleGenre_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instead of looking at bar graphs one can look at violin charts for genres\n",
    "# this is helpful since violin charts show a desnsity along a box=whisker plot which indicates median and quartiles\n",
    "# gives you a smoothed histogram i.e. distribution when violin is turned sideways\n",
    "fig, ax = plt.subplots(figsize=(30,8))\n",
    "plot = sns.violinplot(x=\"genres\", y=\"profit\", data=singleGenre_df,palette='rainbow', ax=ax)\n",
    "lot.set(xlabel = \"genres\", ylabel='profit', title='genre v. profit')\n",
    "plt.savefig('violinplot.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#box and whisker plot: good when looking for outliers \n",
    "fig, ax = plt.subplots(figsize=(40,10))\n",
    "sns.boxplot(x=\"genres\", y=\"profit\", data=singleGenre_df,palette='rainbow', ax=ax)\n",
    "plt.savefig('boxplot.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RIDGEPLOT\n",
    "sns.set(style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)})\n",
    "\n",
    "# Create the data\n",
    "df = singleGenre_df2\n",
    "\n",
    "# Initialize the FacetGrid object\n",
    "pal = sns.cubehelix_palette(10, rot=-.25, light=.7)\n",
    "g = sns.FacetGrid(df, row='genres',hue=\"genres\", aspect=15, height=.5, palette=pal)\n",
    "\n",
    "# Draw the densities in a few steps\n",
    "g.map(sns.kdeplot, \"metascore\", clip_on=False, shade=True, alpha=1, lw=1.5, bw=.2)\n",
    "g.map(sns.kdeplot, \"metascore\", clip_on=False, color=\"w\", lw=2, bw=.2)\n",
    "g.map(plt.axhline, y=0, lw=2, clip_on=False)\n",
    "\n",
    "def label(x, color, label):\n",
    "    ax = plt.gca()\n",
    "    ax.text(0, .2, label, fontweight=\"bold\", color=color,\n",
    "            ha=\"left\", va=\"center\", transform=ax.transAxes)\n",
    "\n",
    "\n",
    "g.map(label, \"metascore\")\n",
    "\n",
    "g.fig.subplots_adjust(hspace=-.25)\n",
    "\n",
    "# Remove axes details that don't play well with overlap\n",
    "g.set_titles('')\n",
    "g.set(yticks=[])\n",
    "g.despine(bottom=True, left=True)\n",
    "plt.savefig('Ridgeplot.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MYSQL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We did not make use of databases but:\n",
    "If one wants to use a database to store the data scraped off the web via mysql:\n",
    "\n",
    "note: one can store to a database especially if the data being scraped is very large, it is more efficient\n",
    "to store to a database and call from the database \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT mysql.connector and connect\n",
    "import mysql.connector\n",
    "\n",
    "mydb = mysql.connector.connect(\n",
    "    host = 'localhost',\n",
    "    user = 'root',\n",
    "    passwd = <your_password>,\n",
    "    database = 'moviedb'\n",
    "    autocommit = True\n",
    ")\n",
    "\n",
    "mycursor = mydb.cursor()\n",
    "mycursor.execute('CREATE DATABASE moviedb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create table: movieInfo \n",
    "mycursor.execute(\"\"\"CREATE TABLE movieInfo (ids VARCHAR(255), \n",
    "                 title VARCHAR(255), genres VARCHAR(255), directors VARCHAR(255), year INTEGER(10))\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add data to table: ids, title, genres, directors, years\n",
    "def insertToMovieInfo(data_list):\n",
    "    '''\n",
    "    Parameters: data_list:<list> list of tuples to append to table\n",
    "    Returns: appends to table\n",
    "    '''\n",
    "    for item in data_list:\n",
    "        \n",
    "        #INSERT INTO tablename, (columns) VALUES (%s)\n",
    "        insert = \"INSERT INTO movieInfo (ids, title, genres, directors, year) VALUES (%s, %s, %s, %s, %s)\"\n",
    "\n",
    "        mycursor.execute(insert, item)\n",
    "        #mydb.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(*) cell 1: the list of movies, top100_10yrs \n",
    "#we take tuple with ids, title, genres, directors, year \n",
    "boxOffice = [x[5] for x in top100_10yrs]\n",
    "listToDf = [(x[0],x[1],x[2],x[3], x[4]) for x in top100_10yrs]\n",
    "\n",
    "insertToMovieInfo(listToDf)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how to retrieve\n",
    "mycursor.execute(\"SELECT * FROM movieInfo\")\n",
    "myresult = mycursor.fetchall()\n",
    "\n",
    "for x in myresult:\n",
    "      print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create separate table for boxoffice \n",
    "mycursor.execute(\"\"\"CREATE TABLE movieBoxoffice (ids, VARCHAR(255), budget VARCHAR(255), \n",
    "                 opening_gross VARCHAR(255), USAgross VARCHAR(255), total_gross VARCHAR(255))\"\"\")\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn boxOffice into tuple\n",
    "boxOffice_tuple = []\n",
    "for item in boxOffice:\n",
    "        ids = item[0]\n",
    "        box_office = item[1]\n",
    "        data = (ids, box_office.get('budget'), box_office.get('openingWeekendUSA'), \n",
    "            box_office.get('grossUSA'), box_office.get('cumulativeWorldwideGross'))\n",
    "        boxOffice_tuple.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert function\n",
    "def insertToMovieBoxOffice(data_list):\n",
    "    '''\n",
    "    Parameters: data_list:<list> list of tuples to append to table\n",
    "    Returns: appends to table\n",
    "    '''\n",
    "    for item in data_list:\n",
    "        insert = \"INSERT INTO movieInfo (ids, title, genres, directors, year) VALUES (%s, %s, %s, %s, %s)\"\n",
    "    mycursor.execute(insert, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert\n",
    "insertToMovieBoxOffice(boxOffice_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create table for scores\n",
    "mycursor.execute(\"\"\"CREATE TABLE movieScores (ids, VARCHAR(255), metascore VARCHAR(255), \n",
    "                 metacritic VARCHAR(255), mrating VARCHAR(255), imdbVotes VARCHAR(255), imdbScore VARCHAR(255)\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert function \n",
    "def insertToMovieScore(data_list):\n",
    "    '''\n",
    "    Parameters: data_list:<list> list of tuples to append to table\n",
    "    Returns: appends to table\n",
    "    '''\n",
    "    for item in data_list:\n",
    "        insert = \"\"\"INSERT INTO movieScores (ids, metascore, metacritic, mrating, imdbVotes, imdbScore) \n",
    "        VALUES (%s, %s, %s, %s, %s)\"\"\"\n",
    "        mycursor.execute(insert, item)\n",
    "#insert         \n",
    "insertToMovieScore(top100_10yrs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create table for votes\n",
    "mycursor.execute(\"\"\"CREATE TABLE movieVotes (ids VARCHAR(255), title VARCHAR(255), one VARCHAR(255), \n",
    "                 two VARCHAR(255), three VARCHAR(255), four VARCHAR(255), five VARCHAR(255), six VARCHAR(255),\n",
    "                 seven VARCHAR(255), eight VARCHAR(255), nine(255), ten(255)\n",
    "                 \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(*) cell three \n",
    "def insertToMovieVotes(data_list):\n",
    "    '''\n",
    "    Parameters: data_list:<list> list of tuples to append to table\n",
    "    Returns: appends to table\n",
    "    '''\n",
    "    for item in data_list:\n",
    "        insert = \"\"\"INSERT INTO movieVotes (ids, title, one, two, three, four, five, six, seven,\n",
    "        eight, nine, ten) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\"\"\"\n",
    "        mycursor.execute(insert, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How to Merge:\n",
    "#note: one could have made a \"mega\" table with ALL information which is better, so no joining is necessary \n",
    "mycursor.execute(\"\"\"SELECT ids FROM movieInfo LEFT JOIN movieBoxOffice ON ids\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dataframe \n",
    "#When one has a large table it is a nice way to select subtables from a large table\n",
    "mycursor.execute(\"SELECT ids,title,genres FROM movieInfo\")\n",
    "myresult = mycursor.fetchall()\n",
    "dataframe = []\n",
    "\n",
    "for x in myresult:\n",
    "    dataframe.append(x)\n",
    "df = pd.DataFrame(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dont forget to close when you're done\n",
    "mydb.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
